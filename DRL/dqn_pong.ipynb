{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dqn_pong.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"zFidzTZlU0XQ","colab_type":"code","colab":{}},"source":["#This code is based on \"Deep RL Hands On\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAfWmuXEd7UJ","colab_type":"code","outputId":"3011b7bd-a2bf-4452-a510-21b2b270883c","executionInfo":{"status":"ok","timestamp":1561631311387,"user_tz":-330,"elapsed":27553,"user":{"displayName":"RUPAM BHATTACHARYYA","photoUrl":"","userId":"08678192808801775482"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fvSc65EuZEiO","colab_type":"code","outputId":"b38ff5d5-02df-4e69-dfd6-8c130b67131b","executionInfo":{"status":"ok","timestamp":1561631267269,"user_tz":-330,"elapsed":7237,"user":{"displayName":"RUPAM BHATTACHARYYA","photoUrl":"","userId":"08678192808801775482"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["pip install import_ipynb"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting import_ipynb\n","  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n","Building wheels for collected packages: import-ipynb\n","  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n","Successfully built import-ipynb\n","Installing collected packages: import-ipynb\n","Successfully installed import-ipynb-0.1.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-k-jDImaj1FV","colab_type":"code","outputId":"bb3ec99b-f39c-4371-c9f9-dbed76beb08f","executionInfo":{"status":"ok","timestamp":1561631318898,"user_tz":-330,"elapsed":1015,"user":{"displayName":"RUPAM BHATTACHARYYA","photoUrl":"","userId":"08678192808801775482"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd \"/content/drive/My Drive/Colab Notebooks\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kwv1ivNeLAXc","colab_type":"code","outputId":"576f88bf-11df-4ff3-ae97-4f50095f25b5","executionInfo":{"status":"ok","timestamp":1561631327449,"user_tz":-330,"elapsed":5646,"user":{"displayName":"RUPAM BHATTACHARYYA","photoUrl":"","userId":"08678192808801775482"}},"colab":{"base_uri":"https://localhost:8080/","height":176}},"source":["pip install tensorboardX"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/57/2f0a46538295b8e7f09625da6dd24c23f9d0d7ef119ca1c33528660130d5/tensorboardX-1.7-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-1.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O3fV3HDbVxY7","colab_type":"code","outputId":"a70ec75f-0d0d-43c6-f2de-16fe0fe160e1","executionInfo":{"status":"error","timestamp":1561640672603,"user_tz":-330,"elapsed":3738,"user":{"displayName":"RUPAM BHATTACHARYYA","photoUrl":"","userId":"08678192808801775482"}},"colab":{"base_uri":"https://localhost:8080/","height":583}},"source":["#!/usr/bin/env python3\n","#from lib import wrappers\n","#from lib import dqn_model\n","import import_ipynb\n","#%cd /content/drive\n","#!ls -l mylib.py\n","import wrappers\n","import DQNmodel\n","\n","import argparse\n","import time\n","import numpy as np\n","import collections\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from tensorboardX import SummaryWriter\n","\n","\n","DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n","MEAN_REWARD_BOUND = 19.5\n","\n","GAMMA = 0.99\n","BATCH_SIZE = 32\n","REPLAY_SIZE = 10000\n","LEARNING_RATE = 1e-4\n","SYNC_TARGET_FRAMES = 1000\n","REPLAY_START_SIZE = 10000\n","\n","EPSILON_DECAY_LAST_FRAME = 10**5\n","EPSILON_START = 1.0\n","EPSILON_FINAL = 0.02\n","\n","\n","Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n","\n","\n","class ExperienceBuffer:\n","    def __init__(self, capacity):\n","        self.buffer = collections.deque(maxlen=capacity)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","    def append(self, experience):\n","        self.buffer.append(experience)\n","\n","    def sample(self, batch_size):\n","        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n","        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n","        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n","               np.array(dones, dtype=np.uint8), np.array(next_states)\n","\n","\n","class Agent:\n","    def __init__(self, env, exp_buffer):\n","        self.env = env\n","        self.exp_buffer = exp_buffer\n","        self._reset()\n","\n","    def _reset(self):\n","        self.state = env.reset()\n","        self.total_reward = 0.0\n","\n","    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n","        done_reward = None\n","\n","        if np.random.random() < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            state_a = np.array([self.state], copy=False)\n","            state_v = torch.tensor(state_a).to(device)\n","            q_vals_v = net(state_v)\n","            _, act_v = torch.max(q_vals_v, dim=1)\n","            action = int(act_v.item())\n","\n","        # do step in the environment\n","        new_state, reward, is_done, _ = self.env.step(action)\n","        self.total_reward += reward\n","\n","        exp = Experience(self.state, action, reward, is_done, new_state)\n","        self.exp_buffer.append(exp)\n","        self.state = new_state\n","        if is_done:\n","            done_reward = self.total_reward\n","            self._reset()\n","        return done_reward\n","\n","\n","def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n","    states, actions, rewards, dones, next_states = batch\n","\n","    states_v = torch.tensor(states).to(device)\n","    next_states_v = torch.tensor(next_states).to(device)\n","    actions_v = torch.tensor(actions).to(device)\n","    rewards_v = torch.tensor(rewards).to(device)\n","    done_mask = torch.ByteTensor(dones).to(device)\n","\n","    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n","    next_state_values = tgt_net(next_states_v).max(1)[0]\n","    next_state_values[done_mask] = 0.0\n","    next_state_values = next_state_values.detach()\n","\n","    expected_state_action_values = next_state_values * GAMMA + rewards_v\n","    return nn.MSELoss()(state_action_values, expected_state_action_values)\n","\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n","    parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n","                        help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n","    parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n","                        help=\"Mean reward boundary for stop of training, default=%.2f\" % MEAN_REWARD_BOUND)\n","    #args = parser.parse_args()\n","    args = parser.parse_known_args()[0]\n","    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","\n","    env = wrappers.make_env(args.env)\n","\n","    #net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n","    net = DQNmodel.DQN(env.observation_space.shape, env.action_space.n).to(device)\n","    #tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n","    tgt_net = DQNmodel.DQN(env.observation_space.shape, env.action_space.n).to(device)\n","    writer = SummaryWriter(comment=\"-\" + args.env)\n","    print(net)\n","\n","    buffer = ExperienceBuffer(REPLAY_SIZE)\n","    agent = Agent(env, buffer)\n","    epsilon = EPSILON_START\n","\n","    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","    total_rewards = []\n","    frame_idx = 0\n","    ts_frame = 0\n","    ts = time.time()\n","    best_mean_reward = None\n","\n","    while True:\n","        frame_idx += 1\n","        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n","\n","        reward = agent.play_step(net, epsilon, device=device)\n","        if reward is not None:\n","            total_rewards.append(reward)\n","            speed = (frame_idx - ts_frame) / (time.time() - ts)\n","            ts_frame = frame_idx\n","            ts = time.time()\n","            mean_reward = np.mean(total_rewards[-100:])\n","            print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n","                frame_idx, len(total_rewards), mean_reward, epsilon,\n","                speed\n","            ))\n","            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n","            writer.add_scalar(\"speed\", speed, frame_idx)\n","            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n","            writer.add_scalar(\"reward\", reward, frame_idx)\n","            if best_mean_reward is None or best_mean_reward < mean_reward:\n","                torch.save(net.state_dict(), args.env + \"-best.dat\")\n","                if best_mean_reward is not None:\n","                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n","                best_mean_reward = mean_reward\n","            if mean_reward > args.reward:\n","                print(\"Solved in %d frames!\" % frame_idx)\n","                break\n","\n","        if len(buffer) < REPLAY_START_SIZE:\n","            continue\n","\n","        if frame_idx % SYNC_TARGET_FRAMES == 0:\n","            tgt_net.load_state_dict(net.state_dict())\n","\n","        optimizer.zero_grad()\n","        batch = buffer.sample(BATCH_SIZE)\n","        loss_t = calc_loss(batch, net, tgt_net, device=device)\n","        loss_t.backward()\n","        optimizer.step()\n","    writer.close()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["DQN(\n","  (conv): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n","    (3): ReLU()\n","    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (5): ReLU()\n","  )\n","  (fc): Sequential(\n","    (0): Linear(in_features=3136, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=6, bias=True)\n","  )\n",")\n","822: done 1 games, mean reward -21.000, eps 0.99, speed 609.36 f/s\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-5ea918e3a710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPSILON_FINAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON_START\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mEPSILON_DECAY_LAST_FRAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-5ea918e3a710>\u001b[0m in \u001b[0;36mplay_step\u001b[0;34m(self, net, epsilon, device)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# do step in the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Colab Notebooks/wrappers.ipynb\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, obs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}